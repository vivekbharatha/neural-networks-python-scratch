{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c9f33d13-d910-4bfa-9ee7-7115348d624e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9.144\n"
     ]
    }
   ],
   "source": [
    "# Single neuron\n",
    "inputs = [1, 2.2, 3.6, 4.9]\n",
    "weights = [0.5, 0.3, 0.84, 0.4]\n",
    "bias = 3\n",
    "\n",
    "output = (\n",
    "    inputs[0] * weights[0]\n",
    "    + inputs[1] * weights[1]\n",
    "    + inputs[2] * weights[2]\n",
    "    + inputs[3] * weights[3]\n",
    "    + bias\n",
    ")\n",
    "print(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ac07dc41-21e5-4f5d-9c6f-3e6470754d53",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[9.144, 7.462, 12.206]\n"
     ]
    }
   ],
   "source": [
    "# 3 neurons layer\n",
    "inputs = [1, 2.2, 3.6, 4.9]\n",
    "weights1 = [0.5, 0.3, 0.84, 0.4]\n",
    "weights2 = [0.11, 0.31, 0.84, 0.54]\n",
    "weights3 = [0.85, 0.23, 0.34, 0.74]\n",
    "bias1 = 3\n",
    "bias2 = 1\n",
    "bias3 = 6\n",
    "\n",
    "output1 = (\n",
    "    inputs[0] * weights1[0]\n",
    "    + inputs[1] * weights1[1]\n",
    "    + inputs[2] * weights1[2]\n",
    "    + inputs[3] * weights1[3]\n",
    "    + bias1\n",
    ")\n",
    "output2 = (\n",
    "    inputs[0] * weights2[0]\n",
    "    + inputs[1] * weights2[1]\n",
    "    + inputs[2] * weights2[2]\n",
    "    + inputs[3] * weights2[3]\n",
    "    + bias2\n",
    ")\n",
    "output3 = (\n",
    "    inputs[0] * weights3[0]\n",
    "    + inputs[1] * weights3[1]\n",
    "    + inputs[2] * weights3[2]\n",
    "    + inputs[3] * weights3[3]\n",
    "    + bias3\n",
    ")\n",
    "\n",
    "print([output1, output2, output3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "93a5c5bf-b9cf-428f-b744-1ae480522518",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[9.144, 8.462, 12.206]\n"
     ]
    }
   ],
   "source": [
    "# Refactoring to 2d matrix - 3 neurons layer\n",
    "inputs = [1, 2.2, 3.6, 4.9]\n",
    "weights = [[0.5, 0.3, 0.84, 0.4], [0.11, 0.31, 0.84, 0.54], [0.85, 0.23, 0.34, 0.74]]\n",
    "bias = [3, 2, 6]\n",
    "\n",
    "layerOutputs = []  # output of this layer of 3 neurons\n",
    "for neuronWeights, neuronBias in zip(weights, bias):\n",
    "    neuronOutput = 0  # output of current neuron\n",
    "    for input, weight in zip(inputs, neuronWeights):\n",
    "        neuronOutput += input * weight\n",
    "    neuronOutput += neuronBias\n",
    "    layerOutputs.append(neuronOutput)\n",
    "\n",
    "print(layerOutputs)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "76504610",
   "metadata": {},
   "source": [
    "# Shapes\n",
    "\n",
    "Array \n",
    "list: l = [1,2,3]\n",
    "Shape = (4,)\n",
    "Type = 1D Array, Vector\n",
    "---------\n",
    "listOfList: lol = \n",
    "[\n",
    "    [1,2,3],\n",
    "    [4,5,6]\n",
    "]\n",
    "Shape = (2,4)\n",
    "Type =  2D Array, Matrix, List of Vectors\n",
    "---------\n",
    "listOfListOfList: lolol = \n",
    "[\n",
    "    [\n",
    "        [1,2,3,3],\n",
    "        [4,5,6,2]\n",
    "    ],\n",
    "    [\n",
    "        [3,7,8,3],\n",
    "        [8,6,5,9]\n",
    "    ],\n",
    "    [\n",
    "        [,4,1,3,1],\n",
    "        [8,9,1,8]\n",
    "    ]\n",
    "]\n",
    "Shape = (3,2,4)\n",
    "Type = 3D Array\n",
    "\n",
    "# Tensor is an object that can be represented as an array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ddff7de5-5de3-417f-a9df-07db489921da",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9.144\n",
      "[ 9.144  8.462 12.206]\n"
     ]
    }
   ],
   "source": [
    "# Dot Product OR Matrix Product\n",
    "import numpy as np\n",
    "\n",
    "# For single neuron\n",
    "inputs = [1.0, 2.2, 3.6, 4.9]\n",
    "weights = [0.5, 0.3, 0.84, 0.4]\n",
    "bias = 3.0\n",
    "\n",
    "output = np.dot(inputs, weights) + bias\n",
    "\n",
    "print(output)\n",
    "\n",
    "# For 3 neurons\n",
    "inputs = [1, 2.2, 3.6, 4.9]\n",
    "weights = [[0.5, 0.3, 0.84, 0.4], [0.11, 0.31, 0.84, 0.54], [0.85, 0.23, 0.34, 0.74]]\n",
    "biases = [3, 2, 6]\n",
    "# output = np.dot(inputs, weights) + biases <-- This will throw error due shapes not aligned (4,) X (3,4)\n",
    "output = np.dot(weights, inputs) + biases  # Shapes are aligned (3,4) X (4,)\n",
    "\n",
    "print(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "05d1a9d3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 9.144  8.462 12.206]\n",
      " [15.614 13.903 19.901]\n",
      " [12.494 10.423 12.501]]\n"
     ]
    }
   ],
   "source": [
    "# Batches (3 samples of 4 inputs) - this allows to calculate in parallel\n",
    "inputs = [[1, 2.2, 3.6, 4.9], [5.1, 3.2, 6.6, 8.9], [3.1, 1.2, 8.6, 0.9]]\n",
    "\n",
    "weights = [[0.5, 0.3, 0.84, 0.4], [0.11, 0.31, 0.84, 0.54], [0.85, 0.23, 0.34, 0.74]]\n",
    "biases = [3, 2, 6]\n",
    "# output = np.dot(inputs, weights) + biases -> Shapes are not aligned (3,4) X (3,4)\n",
    "\n",
    "output = (\n",
    "    np.dot(inputs, np.array(weights).T) + biases\n",
    ")  # Shapes are aligned (3,4) X (4,3)\n",
    "print(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "84336d19-b13d-4805-be91-2a47b7f44c9c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[-0.02518929  0.01000133 -0.10602552]\n",
      " [-0.07703449  0.04588515 -0.1531715 ]\n",
      " [-0.13319932 -0.1075192  -0.02332102]]\n",
      "[[ 0.00102153  0.00137199]\n",
      " [ 0.00228973  0.00258048]\n",
      " [ 0.00230288 -0.00159527]]\n",
      "[[0.00102153 0.00137199]\n",
      " [0.00228973 0.00258048]\n",
      " [0.00230288 0.        ]]\n"
     ]
    }
   ],
   "source": [
    "# Hidden layers with activiation function\n",
    "import numpy as np\n",
    "\n",
    "X = [[1, 2.2, 3.6, 4.9], [5.1, 3.2, 6.6, 8.9], [3.1, 1.2, 8.6, 0.9]]\n",
    "\n",
    "\n",
    "# Layer\n",
    "class LayerDense:\n",
    "    def __init__(self, nInputs, nNeurons):\n",
    "        self.weights = 0.01 * np.random.randn(nInputs, nNeurons)\n",
    "        # Here shape of weights (4,3) is opp to previous block (3,4), reason to avoid extra transpose operation for every forward pass\n",
    "        self.biases = np.zeros((1, nNeurons))\n",
    "\n",
    "    def forward(self, inputs):\n",
    "        self.output = np.dot(inputs, self.weights) + self.biases\n",
    "\n",
    "\n",
    "# ReLU activation\n",
    "class ActivationReLU:\n",
    "    def forward(self, inputs):\n",
    "        self.output = np.maximum(0, inputs)\n",
    "\n",
    "\n",
    "# -----\n",
    "layer1 = LayerDense(4, 3)\n",
    "layer2 = LayerDense(3, 2)\n",
    "activation1 = ActivationReLU()\n",
    "\n",
    "layer1.forward(X)\n",
    "print(layer1.output)\n",
    "\n",
    "layer2.forward(layer1.output)\n",
    "print(layer2.output)\n",
    "\n",
    "activation1.forward(layer2.output)\n",
    "print(activation1.output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "951223ae-df7a-453e-9692-f9ac46b07a9e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0.0000000e+00  0.0000000e+00  0.0000000e+00  0.0000000e+00\n",
      "   0.0000000e+00]\n",
      " [-8.3581581e-05 -7.9040430e-05 -1.3345221e-04  4.6550449e-05\n",
      "   4.5684628e-06]\n",
      " [-2.3999445e-04  5.9346880e-06 -2.2480826e-04  2.0357311e-05\n",
      "   6.1002436e-05]\n",
      " [-4.1212194e-04  4.3767208e-04 -9.5322714e-05 -1.7302230e-04\n",
      "   1.9264895e-04]\n",
      " [-5.5660505e-04  5.2738853e-04 -1.7207881e-04 -2.0267766e-04\n",
      "   2.4708614e-04]]\n",
      "[[0.0000000e+00 0.0000000e+00 0.0000000e+00 0.0000000e+00 0.0000000e+00]\n",
      " [0.0000000e+00 0.0000000e+00 0.0000000e+00 4.6550449e-05 4.5684628e-06]\n",
      " [0.0000000e+00 5.9346880e-06 0.0000000e+00 2.0357311e-05 6.1002436e-05]\n",
      " [0.0000000e+00 4.3767208e-04 0.0000000e+00 0.0000000e+00 1.9264895e-04]\n",
      " [0.0000000e+00 5.2738853e-04 0.0000000e+00 0.0000000e+00 2.4708614e-04]]\n"
     ]
    }
   ],
   "source": [
    "# Hidden layers with activation functions\n",
    "import numpy as np\n",
    "import nnfs\n",
    "from nnfs.datasets import spiral_data\n",
    "\n",
    "nnfs.init()\n",
    "# Using nnfs spiral sample data of 100 samples of 3 different classes\n",
    "X, y = spiral_data(100, 3)\n",
    "\n",
    "\n",
    "# Layer\n",
    "class LayerDense:\n",
    "    def __init__(self, nInputs, nNeurons):\n",
    "        self.weights = 0.01 * np.random.randn(nInputs, nNeurons)\n",
    "        # Here shape of weights (4,3) is opp to previous block (3,4), reason to avoid extra transpose operation for every forward pass\n",
    "        self.biases = np.zeros((1, nNeurons))\n",
    "\n",
    "    def forward(self, inputs):\n",
    "        self.output = np.dot(inputs, self.weights) + self.biases\n",
    "\n",
    "\n",
    "# ReLU activation\n",
    "class ActivationReLU:\n",
    "    def forward(self, inputs):\n",
    "        self.output = np.maximum(0, inputs)\n",
    "\n",
    "\n",
    "# -----\n",
    "layer1 = LayerDense(2, 5)\n",
    "activation1 = ActivationReLU()\n",
    "\n",
    "layer1.forward(X)\n",
    "print(layer1.output[:5])\n",
    "\n",
    "activation1.forward(layer1.output)\n",
    "print(activation1.output[:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f391ed9d-4f92-40ec-b0c1-045e2d9797a7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.33333334 0.33333334 0.33333334]\n",
      " [0.3333332  0.3333332  0.33333364]\n",
      " [0.3333329  0.33333293 0.3333342 ]\n",
      " [0.3333326  0.33333263 0.33333477]\n",
      " [0.33333233 0.3333324  0.33333528]]\n"
     ]
    }
   ],
   "source": [
    "# Softmax\n",
    "\n",
    "# Input -> Exponentiate -> Normalize (Average with diff e^input ) -> Output\n",
    "# Exponentiate + Normalize = Softmax\n",
    "\n",
    "import numpy as np\n",
    "import nnfs\n",
    "from nnfs.datasets import spiral_data\n",
    "\n",
    "nnfs.init()\n",
    "\n",
    "# Layer\n",
    "\n",
    "\n",
    "class LayerDense:\n",
    "    def __init__(self, nInputs, nNeurons):\n",
    "        self.weights = 0.01 * np.random.randn(nInputs, nNeurons)\n",
    "        # Here shape of weights (4,3) is opp to previous block (3,4), reason to avoid extra transpose operation for every forward pass\n",
    "        self.biases = np.zeros((1, nNeurons))\n",
    "\n",
    "    def forward(self, inputs):\n",
    "        self.output = np.dot(inputs, self.weights) + self.biases\n",
    "\n",
    "\n",
    "# ReLU activation\n",
    "class ActivationReLU:\n",
    "    def forward(self, inputs):\n",
    "        self.output = np.maximum(0, inputs)\n",
    "\n",
    "\n",
    "# Softmax activation\n",
    "class ActivationSoftmax:\n",
    "    def forward(self, inputs):\n",
    "        # axis: 0 -> columns, 1 -> rows\n",
    "        expValues = np.exp(inputs - np.max(inputs, axis=1, keepdims=True))\n",
    "        probabilities = expValues / np.sum(expValues, axis=1, keepdims=True)\n",
    "        self.output = probabilities\n",
    "\n",
    "\n",
    "# -----\n",
    "# Using nnfs spiral sample data of 100 samples of 3 different classes\n",
    "X, y = spiral_data(100, 3)\n",
    "layer1 = LayerDense(2, 3)\n",
    "activation1 = ActivationReLU()\n",
    "\n",
    "layer2 = LayerDense(3, 3)\n",
    "activation2 = ActivationSoftmax()\n",
    "\n",
    "layer1.forward(X)\n",
    "activation1.forward(layer1.output)\n",
    "layer2.forward(activation1.output)\n",
    "activation2.forward(layer2.output)\n",
    "print(activation2.output[:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "5c1575bd-360e-4494-b908-23c8d7d500d0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.35667494393873245\n",
      "0.35667494393873245\n"
     ]
    }
   ],
   "source": [
    "# Calculating loss with categorical cross entropy\n",
    "# -ve sum of the target value multiplied by the log of predicted value for each value in the distribution\n",
    "import math\n",
    "\n",
    "softmaxOutput = [0.7, 0.1, 0.2]  # probability / confidence\n",
    "targetOutput = [1, 0, 0]  # One hot vector\n",
    "\n",
    "loss = -(\n",
    "    math.log(softmaxOutput[0]) * targetOutput[0]\n",
    "    + math.log(softmaxOutput[1]) * targetOutput[1]\n",
    "    + math.log(softmaxOutput[2]) * targetOutput[2]\n",
    ")\n",
    "\n",
    "print(loss)\n",
    "\n",
    "loss = -math.log(softmaxOutput[0])\n",
    "print(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "6d17721b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.33333334 0.33333334 0.33333334]\n",
      " [0.3333332  0.3333332  0.33333364]\n",
      " [0.3333329  0.33333293 0.3333342 ]\n",
      " [0.3333326  0.33333263 0.33333477]\n",
      " [0.33333233 0.3333324  0.33333528]]\n",
      "Loss: 1.0986104\n"
     ]
    }
   ],
   "source": [
    "# Implementing Loss\n",
    "\n",
    "import numpy as np\n",
    "import nnfs\n",
    "from nnfs.datasets import spiral_data\n",
    "\n",
    "nnfs.init()\n",
    "\n",
    "# Layer\n",
    "\n",
    "\n",
    "class LayerDense:\n",
    "    def __init__(self, nInputs, nNeurons):\n",
    "        self.weights = 0.01 * np.random.randn(nInputs, nNeurons)\n",
    "        # Here shape of weights (4,3) is opp to previous block (3,4), reason to avoid extra transpose operation for every forward pass\n",
    "        self.biases = np.zeros((1, nNeurons))\n",
    "\n",
    "    def forward(self, inputs):\n",
    "        self.output = np.dot(inputs, self.weights) + self.biases\n",
    "\n",
    "\n",
    "# ReLU activation\n",
    "class ActivationReLU:\n",
    "    def forward(self, inputs):\n",
    "        self.output = np.maximum(0, inputs)\n",
    "\n",
    "\n",
    "# Softmax activation\n",
    "class ActivationSoftmax:\n",
    "    def forward(self, inputs):\n",
    "        # axis: 0 -> columns, 1 -> rows\n",
    "        expValues = np.exp(inputs - np.max(inputs, axis=1, keepdims=True))\n",
    "        probabilities = expValues / np.sum(expValues, axis=1, keepdims=True)\n",
    "        self.output = probabilities\n",
    "\n",
    "\n",
    "# Loss\n",
    "class Loss:\n",
    "    def calculate(self, output, y):\n",
    "        sampleLosses = self.forward(output, y)\n",
    "        dataLoss = np.mean(sampleLosses)\n",
    "        return dataLoss\n",
    "\n",
    "\n",
    "class LossCategoricalCrossEntropy(Loss):\n",
    "    def forward(self, yPred, yTrue):\n",
    "        samples = len(yPred)\n",
    "        yPredClipped = np.clip(yPred, 1e-7, 1 - 1e-7)  # to prevent inf mean\n",
    "        if len(yTrue.shape) == 1:  # Vector Eg: [0,1,1]\n",
    "            correctConfidences = yPredClipped[range(samples), yTrue]\n",
    "        elif (\n",
    "            len(yTrue.shape) == 2\n",
    "        ):  # List of one hot encoded values Eg: [[1,0,0],[0,1,0],[0,1,0]]\n",
    "            correctConfidences = np.sum(yPredClipped * yTrue, axis=1)\n",
    "\n",
    "        negativeLogLikelihoods = -np.log(correctConfidences)\n",
    "        return negativeLogLikelihoods\n",
    "\n",
    "\n",
    "# -----\n",
    "# Using nnfs spiral sample data of 100 samples of 3 different classes\n",
    "X, y = spiral_data(100, 3)\n",
    "layer1 = LayerDense(2, 3)\n",
    "activation1 = ActivationReLU()\n",
    "\n",
    "layer2 = LayerDense(3, 3)\n",
    "activation2 = ActivationSoftmax()\n",
    "\n",
    "layer1.forward(X)\n",
    "activation1.forward(layer1.output)\n",
    "layer2.forward(activation1.output)\n",
    "activation2.forward(layer2.output)\n",
    "print(activation2.output[:5])\n",
    "\n",
    "lossFunction = LossCategoricalCrossEntropy()\n",
    "loss = lossFunction.calculate(activation2.output, y)\n",
    "print(\"Loss:\", loss)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "dd63e49c",
   "metadata": {},
   "source": [
    "# Derivatives\n",
    "\n",
    "## Numerical derivatives - D\n",
    "calculating the slope of the tangent line using two infinitely close points, or as with the code solution — \n",
    "calculating the slope of a tangent line made from two points that were \"sufficiently close\"\n",
    "\n",
    "This won't be the ideal way as we need to calculate for each weight and bias across neurons in all layers which\n",
    "is kind of a brute force\n",
    "\n",
    "The better approach is using analytical derivative with chain rule\n",
    "\n",
    "Rules:\n",
    "- Derivative of a constant multiple of the function equals the constant multiple of the function's derivative:\n",
    "    `D[k*f(x)] = k * D[f(x)]`\n",
    "\n",
    "- Derivative of a sum/subtraction of functions equals the sum of their derivatives:\n",
    "    `D[f(x) + g(x)] = D[f(x)] + D[g(x)] , D[f(x) - g(x)] = D[f(x)] - D[g(x)]`\n",
    "\n",
    "- Derivative of an exponentiation:\n",
    "    `D[x^n] = n * x^(n-1)`\n",
    "\n",
    "\n",
    "# Partial Derivatives - d\n",
    "The partial derivative measures how much impact a single input has on a function’s output.\n",
    "\n",
    "Rules:\n",
    "- The partial derivative of the sum with respect to any input equals 1: \n",
    "    `f(x,y) = x + y -> dx[f(x,y)] = 1, dy[f(x,y)] = 1`\n",
    "\n",
    "- The partial derivative of the multiplication operation with 2 inputs, with respect to any input, equals the other input:\n",
    "    `f(x,y) = x * y -> dx[f(x,y)] = y, dy[f(x,y)] = x`\n",
    "\n",
    "- The partial derivative of the max function of 2 variables with respect to any of them is 1 if this variable is the biggest and 0 otherwise:\n",
    "    `f(x,y) = max(x,y) -> dx[f(x,y)] = 1(x>y)`\n",
    "\n",
    "- The derivative of the max function of a single variable and 0 equals 1 if the variable is greater than 0 and 0 otherwise:\n",
    "    `f(x) = max(x,0) -> dx[f(x,y)] = 1 (x>0)`\n",
    "\n",
    "- The derivative of chained functions equals the product of the partial derivatives of the subsequent functions:\n",
    "    `Dx[f(g(x))] = Dg(x)[f(g(x))] * Dx[g(x)]`\n",
    "\n",
    "- The same applies to the partial derivatives:\n",
    "    `dx[f(g(y,h(x,z)))] = dg(y,h(x,z))[f(g(y,h(x,z)))] * d(y,h(x,z))[g(y,h(x,z))] * d(x,z)[h(x,z)]`\n",
    "\n",
    "- The gradient is a vector of all possible partial derivatives. An example of a triple-input function:\n",
    "    ```\n",
    "    G[f(x,y,z)] = [ \n",
    "                    dx[f(x,y,z)] \n",
    "                    dy[f(x,y,z)] \n",
    "                    dz[f(x,y,z)] \n",
    "                ] = [\n",
    "                    dx\n",
    "                    dy\n",
    "                    dz\n",
    "                ] f(x,y,z)\n",
    "    ```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "9fe83d16",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-3.0 2.0 6.0 1.0\n",
      "6.0\n",
      "Output: 6.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "-3.0 1.0 -1.0 -2.0 2.0 3.0\n",
      "[-3.0, -1.0, 2.0] [1.0, -2.0, 3.0] 1.0\n",
      "[-3.0, -1.0, 2.0] 1.0\n",
      "[-3.001, -0.998, 1.997] 0.999\n",
      "Output: 5.985\n"
     ]
    }
   ],
   "source": [
    "# Backpropagation for a single neuron\n",
    "\n",
    "# Forward pass\n",
    "x = [1.0, -2.0, 3.0]\n",
    "w = [-3.0, -1.0, 2.0]\n",
    "b = 1.0\n",
    "\n",
    "xw0 = x[0] * w[0]\n",
    "xw1 = x[1] * w[1]\n",
    "xw2 = x[2] * w[2]\n",
    "\n",
    "print(xw0, xw1, xw2, b)\n",
    "\n",
    "z = xw0 + xw1 + xw2 + b\n",
    "print(z)\n",
    "\n",
    "# ReLU activation function\n",
    "y = max(z, 0)\n",
    "print(\"Output:\", y)\n",
    "\n",
    "# The first step is to backpropagate our gradients by calculating derivatives and partial derivatives\n",
    "# with respect to each of our parameters and inputs. To do this, we’re going to use the chain rule.\n",
    "\n",
    "# Backward pass\n",
    "# The derivative from the next layer\n",
    "dvalue = 1.0\n",
    "\n",
    "# Derivative of ReLU and the chain rule\n",
    "drelu_dz = dvalue * (1.0 if z > 0 else 0.0)\n",
    "print(drelu_dz)\n",
    "\n",
    "# Moving backward through our neural network, next is sum of weighted inputs + bias\n",
    "\n",
    "# Partial derivatives of the multiplication, the chain rule\n",
    "dsum_dxw0 = 1\n",
    "drelu_dxw0 = drelu_dz * dsum_dxw0\n",
    "print(drelu_dxw0)\n",
    "\n",
    "dsum_dxw1 = 1\n",
    "drelu_dxw1 = drelu_dz * dsum_dxw1\n",
    "print(drelu_dxw1)\n",
    "\n",
    "dsum_dxw2 = 1\n",
    "drelu_dxw2 = drelu_dz * dsum_dxw2\n",
    "print(drelu_dxw2)\n",
    "\n",
    "dsum_db = 1\n",
    "drelu_db = drelu_dz * dsum_db\n",
    "print(drelu_db)\n",
    "\n",
    "# Partial derivatives of the multiplication, the chain rule\n",
    "dmul_dx0 = w[0]\n",
    "dmul_dx1 = w[1]\n",
    "dmul_dx2 = w[2]\n",
    "dmul_dw0 = x[0]\n",
    "dmul_dw1 = x[1]\n",
    "dmul_dw2 = x[2]\n",
    "\n",
    "drelu_dx0 = drelu_dxw0 * dmul_dx0\n",
    "drelu_dw0 = drelu_dxw0 * dmul_dw0\n",
    "drelu_dx1 = drelu_dxw1 * dmul_dx1\n",
    "drelu_dw1 = drelu_dxw1 * dmul_dw1\n",
    "drelu_dx2 = drelu_dxw2 * dmul_dx2\n",
    "drelu_dw2 = drelu_dxw2 * dmul_dw2\n",
    "\n",
    "print(drelu_dx0, drelu_dw0, drelu_dx1, drelu_dw1, drelu_dx2, drelu_dw2)\n",
    "\n",
    "# All together, the partial derivatives above, combined into a vector, make up our gradients.\n",
    "# Our gradients could be represented as:\n",
    "dx = [drelu_dx0, drelu_dx1, drelu_dx2]  # gradients on inputs\n",
    "dw = [drelu_dw0, drelu_dw1, drelu_dw2]  # gradients on weights\n",
    "db = drelu_db  # gradient on bias...just 1 bias here\n",
    "\n",
    "print(dx, dw, db)\n",
    "print(w, b)\n",
    "\n",
    "# Applying fraction of gradients to minimize\n",
    "w[0] += -0.001 * dw[0]\n",
    "w[1] += -0.001 * dw[1]\n",
    "w[2] += -0.001 * dw[2]\n",
    "b += -0.001 * db\n",
    "\n",
    "print(w, b)\n",
    "\n",
    "# Forward pass with new weights and bias\n",
    "# Multiplying inputs by weights\n",
    "xw0 = x[0] * w[0]\n",
    "xw1 = x[1] * w[1]\n",
    "xw2 = x[2] * w[2]\n",
    "\n",
    "# Adding\n",
    "z = xw0 + xw1 + xw2 + b\n",
    "# ReLU activation function\n",
    "y = max(z, 0)\n",
    "print(\"Output:\", y)  # 5.985\n",
    "\n",
    "# We’ve successfully decreased this neuron’s output from 6.000 to 5.985. Note that it does not\n",
    "# make sense to decrease the neuron’s output in a real neural network; we were doing this purely\n",
    "# as a simpler exercise than the full network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "873fc014",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 0.44 -0.38 -0.07  1.37]\n",
      "[ 0.44 -0.38 -0.07  1.37]\n",
      "[ 0.44 -0.38 -0.07  1.37]\n",
      "[[ 0.44 -0.38 -0.07  1.37]\n",
      " [ 0.88 -0.76 -0.14  2.74]\n",
      " [ 1.32 -1.14 -0.21  4.11]]\n",
      "[[ 0.5  0.5  0.5]\n",
      " [20.1 20.1 20.1]\n",
      " [10.9 10.9 10.9]\n",
      " [ 4.1  4.1  4.1]]\n",
      "[[6. 6. 6.]]\n",
      "[[1 1 0 0]\n",
      " [1 0 0 1]\n",
      " [0 1 1 0]]\n",
      "[[ 1  2  0  0]\n",
      " [ 5  0  0  8]\n",
      " [ 0 10 11  0]]\n",
      "[[ 1  2  0  0]\n",
      " [ 5  0  0  8]\n",
      " [ 0 10 11  0]]\n"
     ]
    }
   ],
   "source": [
    "# Deriving from scratch\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "# Passed-in gradient from the next layer\n",
    "# for the purpose of this example we're going to use\n",
    "# a vector of 1s\n",
    "dValues = np.array([[1.0, 1.0, 1.0]])\n",
    "\n",
    "# We have 3 sets of weights - one set for each neuron\n",
    "# we have 4 inputs, thus 4 weights\n",
    "# recall that we keep weights transposed\n",
    "weights = np.array(\n",
    "    [[0.2, 0.8, -0.5, 1], [0.5, -0.91, 0.26, -0.5], [-0.26, -0.27, 0.17, 0.87]]\n",
    ").T\n",
    "\n",
    "# Sum weights related to the given input multiplied by\n",
    "# the gradient related to the given neuron\n",
    "# the partial derivative with respect to the input equals the related weight\n",
    "dX0 = sum(\n",
    "    [\n",
    "        weights[0][0] * dValues[0][0],\n",
    "        weights[0][1] * dValues[0][1],\n",
    "        weights[0][2] * dValues[0][2],\n",
    "    ]\n",
    ")\n",
    "dX1 = sum(\n",
    "    [\n",
    "        weights[1][0] * dValues[0][0],\n",
    "        weights[1][1] * dValues[0][1],\n",
    "        weights[1][2] * dValues[0][2],\n",
    "    ]\n",
    ")\n",
    "dX2 = sum(\n",
    "    [\n",
    "        weights[2][0] * dValues[0][0],\n",
    "        weights[2][1] * dValues[0][1],\n",
    "        weights[2][2] * dValues[0][2],\n",
    "    ]\n",
    ")\n",
    "dX3 = sum(\n",
    "    [\n",
    "        weights[3][0] * dValues[0][0],\n",
    "        weights[3][1] * dValues[0][1],\n",
    "        weights[3][2] * dValues[0][2],\n",
    "    ]\n",
    ")\n",
    "\n",
    "dInputs = np.array([dX0, dX1, dX2, dX3])\n",
    "print(dInputs)\n",
    "\n",
    "# -------------------------------------------------\n",
    "\n",
    "# As both dValues and weights are numpy arrays, we can simplify the math\n",
    "\n",
    "# Sum weights related to the given input multiplied by\n",
    "# the gradient related to the given neuron\n",
    "dx0 = sum(weights[0] * dValues[0])\n",
    "dx1 = sum(weights[1] * dValues[0])\n",
    "dx2 = sum(weights[2] * dValues[0])\n",
    "dx3 = sum(weights[3] * dValues[0])\n",
    "dInputs = np.array([dx0, dx1, dx2, dx3])\n",
    "print(dInputs)\n",
    "\n",
    "# We can simplify further by direct dot product\n",
    "dInputs = np.dot(dValues[0], weights.T)\n",
    "print(dInputs)\n",
    "\n",
    "# -------------------------------------------------\n",
    "\n",
    "# Improve to support batch of samples instead of single sample\n",
    "\n",
    "dValues = np.array([[1.0, 1.0, 1.0], [2.0, 2.0, 2.0], [3.0, 3.0, 3.0]])\n",
    "\n",
    "dInputs = np.dot(dValues, weights.T)\n",
    "print(dInputs)\n",
    "\n",
    "# -------------------------------------------------\n",
    "\n",
    "# To calculate dWeights\n",
    "\n",
    "# Passed-in gradient from the next layer\n",
    "# for the purpose of this example we're going to use\n",
    "# an array of an incremental gradient values\n",
    "dValues = np.array([[1.0, 1.0, 1.0], [2.0, 2.0, 2.0], [3.0, 3.0, 3.0]])\n",
    "\n",
    "# We have 3 sets of inputs - samples\n",
    "inputs = np.array([[1, 2, 3, 2.5], [2.0, 5.0, -1.0, 2], [-1.5, 2.7, 3.3, -0.8]])\n",
    "\n",
    "# sum weights of given input\n",
    "# and multiply by the passed-in gradient for this neuron\n",
    "dWeights = np.dot(inputs.T, dValues)\n",
    "print(dWeights)\n",
    "\n",
    "# -------------------------------------------------\n",
    "\n",
    "# To calculate dBiases\n",
    "\n",
    "# One bias for each neuron\n",
    "# biases are the row vector with a shape (1, neurons)\n",
    "biases = np.array([[2, 3, 0.5]])\n",
    "\n",
    "# dBiases - sum values, do this over samples (first axis), keepdims\n",
    "# since this by default will produce a plain list -\n",
    "dBiases = np.sum(dValues, axis=0, keepdims=True)\n",
    "print(dBiases)\n",
    "\n",
    "# -------------------------------------------------\n",
    "\n",
    "# To calculate dRelu\n",
    "\n",
    "# Example layer output\n",
    "z = np.array([[1, 2, -3, -4], [2, -7, -1, 3], [-1, 2, 5, -1]])\n",
    "\n",
    "dValues = np.array([[1, 2, 3, 4], [5, 6, 7, 8], [9, 10, 11, 12]])\n",
    "\n",
    "# ReLU activation's derivative\n",
    "dRelu = np.zeros_like(z)\n",
    "dRelu[z > 0] = 1\n",
    "\n",
    "print(dRelu)\n",
    "\n",
    "# Chain rule\n",
    "dRelu *= dValues\n",
    "print(dRelu)\n",
    "\n",
    "# We can improve this as -\n",
    "# Since the ReLU() derivative array is filled with 1s, which do\n",
    "# not change the values multiplied by them, and 0s that zero the multiplying value, this means that\n",
    "# we can take the gradients of the subsequent function and set to 0 all of the values that correspond\n",
    "# to the ReLU() input and are equal to or less than 0\n",
    "\n",
    "# ReLU activation's derivative\n",
    "# with the chain rule applied\n",
    "dRelu = dValues.copy()\n",
    "dRelu[z <= 0] = 0\n",
    "print(dRelu)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "e3fe76f3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0.179515   0.5003665 -0.262746 ]\n",
      " [ 0.742093  -0.9152577 -0.2758402]\n",
      " [-0.510153   0.2529017  0.1629592]\n",
      " [ 0.971328  -0.5021842  0.8636583]]\n",
      "[[1.994 2.994 0.494]]\n"
     ]
    }
   ],
   "source": [
    "# Full code for forward and backward pass of a single neuron with a full layer and batch based partial derivatives\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "# Passed-in gradient from the next layer\n",
    "# for the purpose of this example we're going to use\n",
    "# an array of an incremental gradient values\n",
    "dValues = np.array([[1.0, 1.0, 1.0], [2.0, 2.0, 2.0], [3.0, 3.0, 3.0]])\n",
    "\n",
    "# We have 3 sets of inputs - samples\n",
    "inputs = np.array([[1, 2, 3, 2.5], [2.0, 5.0, -1.0, 2], [-1.5, 2.7, 3.3, -0.8]])\n",
    "\n",
    "# We have 3 sets of weights - one set for each neuron\n",
    "# we have 4 inputs, thus 4 weights\n",
    "# recall that we keep weights transposed\n",
    "weights = np.array(\n",
    "    [[0.2, 0.8, -0.5, 1], [0.5, -0.91, 0.26, -0.5], [-0.26, -0.27, 0.17, 0.87]]\n",
    ").T\n",
    "\n",
    "# One bias for each neuron\n",
    "# biases are the row vector with a shape (1, neurons)\n",
    "biases = np.array([[2, 3, 0.5]])\n",
    "\n",
    "# Forward pass\n",
    "layerOutputs = np.dot(inputs, weights) + biases  # Dense layer\n",
    "reluOutputs = np.maximum(0, layerOutputs)  # ReLU activation\n",
    "\n",
    "# Let's optimize and test backpropagation here\n",
    "# ReLU activation - simulates derivative with respect to input values\n",
    "# from next layer passed to current layer during backpropagation\n",
    "dRelu = reluOutputs.copy()\n",
    "dRelu[layerOutputs <= 0] = 0\n",
    "\n",
    "# Dense layer\n",
    "# dInputs - multiply by weights\n",
    "dInputs = np.dot(dRelu, weights.T)\n",
    "\n",
    "# dWeights - multiply by inputs\n",
    "dWeights = np.dot(inputs.T, dRelu)\n",
    "\n",
    "# dbiases - sum values, do this over samples (first axis), keepdims\n",
    "# since this by default will produce a plain list\n",
    "dbiases = np.sum(dRelu, axis=0, keepdims=True)\n",
    "\n",
    "# Updated parameters\n",
    "weights += -0.001 * dWeights\n",
    "biases += -0.001 * dBiases\n",
    "\n",
    "print(weights)\n",
    "print(biases)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
