{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c9f33d13-d910-4bfa-9ee7-7115348d624e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9.144\n"
     ]
    }
   ],
   "source": [
    "# Single neuron\n",
    "inputs = [1, 2.2, 3.6, 4.9]\n",
    "weights = [0.5, 0.3, 0.84, 0.4]\n",
    "bias = 3\n",
    "\n",
    "output = (\n",
    "    inputs[0] * weights[0]\n",
    "    + inputs[1] * weights[1]\n",
    "    + inputs[2] * weights[2]\n",
    "    + inputs[3] * weights[3]\n",
    "    + bias\n",
    ")\n",
    "print(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ac07dc41-21e5-4f5d-9c6f-3e6470754d53",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[9.144, 7.462, 12.206]\n"
     ]
    }
   ],
   "source": [
    "# 3 neurons layer\n",
    "inputs = [1, 2.2, 3.6, 4.9]\n",
    "weights1 = [0.5, 0.3, 0.84, 0.4]\n",
    "weights2 = [0.11, 0.31, 0.84, 0.54]\n",
    "weights3 = [0.85, 0.23, 0.34, 0.74]\n",
    "bias1 = 3\n",
    "bias2 = 1\n",
    "bias3 = 6\n",
    "\n",
    "output1 = (\n",
    "    inputs[0] * weights1[0]\n",
    "    + inputs[1] * weights1[1]\n",
    "    + inputs[2] * weights1[2]\n",
    "    + inputs[3] * weights1[3]\n",
    "    + bias1\n",
    ")\n",
    "output2 = (\n",
    "    inputs[0] * weights2[0]\n",
    "    + inputs[1] * weights2[1]\n",
    "    + inputs[2] * weights2[2]\n",
    "    + inputs[3] * weights2[3]\n",
    "    + bias2\n",
    ")\n",
    "output3 = (\n",
    "    inputs[0] * weights3[0]\n",
    "    + inputs[1] * weights3[1]\n",
    "    + inputs[2] * weights3[2]\n",
    "    + inputs[3] * weights3[3]\n",
    "    + bias3\n",
    ")\n",
    "\n",
    "print([output1, output2, output3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "93a5c5bf-b9cf-428f-b744-1ae480522518",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[9.144, 8.462, 12.206]\n"
     ]
    }
   ],
   "source": [
    "# Refactoring to 2d matrix - 3 neurons layer\n",
    "inputs = [1, 2.2, 3.6, 4.9]\n",
    "weights = [[0.5, 0.3, 0.84, 0.4], [0.11, 0.31, 0.84, 0.54], [0.85, 0.23, 0.34, 0.74]]\n",
    "bias = [3, 2, 6]\n",
    "\n",
    "layerOutputs = []  # output of this layer of 3 neurons\n",
    "for neuronWeights, neuronBias in zip(weights, bias):\n",
    "    neuronOutput = 0  # output of current neuron\n",
    "    for input, weight in zip(inputs, neuronWeights):\n",
    "        neuronOutput += input * weight\n",
    "    neuronOutput += neuronBias\n",
    "    layerOutputs.append(neuronOutput)\n",
    "\n",
    "print(layerOutputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "a49d0cc7-7bd8-46a5-a875-1203278b8381",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nArray \\nlist: l = [1,2,3]\\nShape = (4,)\\nType = 1D Array, Vector\\n---------\\nlistOfList: lol = \\n[\\n    [1,2,3],\\n    [4,5,6]\\n]\\nShape = (2,4)\\nType =  2D Array, Matrix, List of Vectors\\n---------\\nlistOfListOfList: lolol = \\n[\\n    [\\n        [1,2,3,3],\\n        [4,5,6,2]\\n    ],\\n    [\\n        [3,7,8,3],\\n        [8,6,5,9]\\n    ],\\n    [\\n        [,4,1,3,1],\\n        [8,9,1,8]\\n    ]\\n]\\nShape = (3,2,4)\\nType = 3D Array\\n'"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Shapes\n",
    "\n",
    "\"\"\"\n",
    "Array \n",
    "list: l = [1,2,3]\n",
    "Shape = (4,)\n",
    "Type = 1D Array, Vector\n",
    "---------\n",
    "listOfList: lol = \n",
    "[\n",
    "    [1,2,3],\n",
    "    [4,5,6]\n",
    "]\n",
    "Shape = (2,4)\n",
    "Type =  2D Array, Matrix, List of Vectors\n",
    "---------\n",
    "listOfListOfList: lolol = \n",
    "[\n",
    "    [\n",
    "        [1,2,3,3],\n",
    "        [4,5,6,2]\n",
    "    ],\n",
    "    [\n",
    "        [3,7,8,3],\n",
    "        [8,6,5,9]\n",
    "    ],\n",
    "    [\n",
    "        [,4,1,3,1],\n",
    "        [8,9,1,8]\n",
    "    ]\n",
    "]\n",
    "Shape = (3,2,4)\n",
    "Type = 3D Array\n",
    "\"\"\"\n",
    "# Tensor is an object that can be represented as an array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ddff7de5-5de3-417f-a9df-07db489921da",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9.144\n",
      "[ 9.144  8.462 12.206]\n"
     ]
    }
   ],
   "source": [
    "# Dot Product OR Matrix Product\n",
    "import numpy as np\n",
    "\n",
    "# For single neuron\n",
    "inputs = [1.0, 2.2, 3.6, 4.9]\n",
    "weights = [0.5, 0.3, 0.84, 0.4]\n",
    "bias = 3.0\n",
    "\n",
    "output = np.dot(inputs, weights) + bias\n",
    "\n",
    "print(output)\n",
    "\n",
    "# For 3 neurons\n",
    "inputs = [1, 2.2, 3.6, 4.9]\n",
    "weights = [[0.5, 0.3, 0.84, 0.4], [0.11, 0.31, 0.84, 0.54], [0.85, 0.23, 0.34, 0.74]]\n",
    "biases = [3, 2, 6]\n",
    "# output = np.dot(inputs, weights) + biases <-- This will throw error due shapes not aligned (4,) X (3,4)\n",
    "output = np.dot(weights, inputs) + biases  # Shapes are aligned (3,4) X (4,)\n",
    "\n",
    "print(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "05d1a9d3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 9.144  8.462 12.206]\n",
      " [15.614 13.903 19.901]\n",
      " [12.494 10.423 12.501]]\n"
     ]
    }
   ],
   "source": [
    "# Batches (3 samples of 4 inputs) - this allows to calculate in parallel\n",
    "inputs = [[1, 2.2, 3.6, 4.9], [5.1, 3.2, 6.6, 8.9], [3.1, 1.2, 8.6, 0.9]]\n",
    "\n",
    "weights = [[0.5, 0.3, 0.84, 0.4], [0.11, 0.31, 0.84, 0.54], [0.85, 0.23, 0.34, 0.74]]\n",
    "biases = [3, 2, 6]\n",
    "# output = np.dot(inputs, weights) + biases -> Shapes are not aligned (3,4) X (3,4)\n",
    "\n",
    "output = (\n",
    "    np.dot(inputs, np.array(weights).T) + biases\n",
    ")  # Shapes are aligned (3,4) X (4,3)\n",
    "print(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "84336d19-b13d-4805-be91-2a47b7f44c9c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0.06575596 -0.01828519  0.00352017]\n",
      " [ 0.09471591 -0.03720227  0.01446798]\n",
      " [ 0.12521979 -0.02337999  0.02267963]]\n",
      "[[0.00048866 0.00016374]\n",
      " [0.00066632 0.0003306 ]\n",
      " [0.0013075  0.00021344]]\n",
      "[[0.00048866 0.00016374]\n",
      " [0.00066632 0.0003306 ]\n",
      " [0.0013075  0.00021344]]\n"
     ]
    }
   ],
   "source": [
    "# Hidden layers with activiation function\n",
    "import numpy as np\n",
    "\n",
    "X = [[1, 2.2, 3.6, 4.9], [5.1, 3.2, 6.6, 8.9], [3.1, 1.2, 8.6, 0.9]]\n",
    "\n",
    "\n",
    "# Layer\n",
    "class LayerDense:\n",
    "    def __init__(self, nInputs, nNeurons):\n",
    "        self.weights = 0.01 * np.random.randn(nInputs, nNeurons)\n",
    "        # Here shape of weights (4,3) is opp to previous block (3,4), reason to avoid extra transpose operation for every forward pass\n",
    "        self.biases = np.zeros((1, nNeurons))\n",
    "\n",
    "    def forward(self, inputs):\n",
    "        self.output = np.dot(inputs, self.weights) + self.biases\n",
    "\n",
    "\n",
    "# ReLU activation\n",
    "class ActivationReLU:\n",
    "    def forward(self, inputs):\n",
    "        self.output = np.maximum(0, inputs)\n",
    "\n",
    "\n",
    "# -----\n",
    "layer1 = LayerDense(4, 3)\n",
    "layer2 = LayerDense(3, 2)\n",
    "activation1 = ActivationReLU()\n",
    "\n",
    "layer1.forward(X)\n",
    "print(layer1.output)\n",
    "\n",
    "layer2.forward(layer1.output)\n",
    "print(layer2.output)\n",
    "\n",
    "activation1.forward(layer2.output)\n",
    "print(activation1.output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "951223ae-df7a-453e-9692-f9ac46b07a9e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0.0000000e+00  0.0000000e+00  0.0000000e+00  0.0000000e+00\n",
      "   0.0000000e+00]\n",
      " [-8.3581581e-05 -7.9040430e-05 -1.3345221e-04  4.6550449e-05\n",
      "   4.5684628e-06]\n",
      " [-2.3999445e-04  5.9346880e-06 -2.2480826e-04  2.0357311e-05\n",
      "   6.1002436e-05]\n",
      " [-4.1212194e-04  4.3767208e-04 -9.5322714e-05 -1.7302230e-04\n",
      "   1.9264895e-04]\n",
      " [-5.5660505e-04  5.2738853e-04 -1.7207881e-04 -2.0267766e-04\n",
      "   2.4708614e-04]]\n",
      "[[0.0000000e+00 0.0000000e+00 0.0000000e+00 0.0000000e+00 0.0000000e+00]\n",
      " [0.0000000e+00 0.0000000e+00 0.0000000e+00 4.6550449e-05 4.5684628e-06]\n",
      " [0.0000000e+00 5.9346880e-06 0.0000000e+00 2.0357311e-05 6.1002436e-05]\n",
      " [0.0000000e+00 4.3767208e-04 0.0000000e+00 0.0000000e+00 1.9264895e-04]\n",
      " [0.0000000e+00 5.2738853e-04 0.0000000e+00 0.0000000e+00 2.4708614e-04]]\n"
     ]
    }
   ],
   "source": [
    "# Hidden layers with activation functions\n",
    "import numpy as np\n",
    "import nnfs\n",
    "from nnfs.datasets import spiral_data\n",
    "\n",
    "nnfs.init()\n",
    "# Using nnfs spiral sample data of 100 samples of 3 different classes\n",
    "X, y = spiral_data(100, 3)\n",
    "\n",
    "\n",
    "# Layer\n",
    "class LayerDense:\n",
    "    def __init__(self, nInputs, nNeurons):\n",
    "        self.weights = 0.01 * np.random.randn(nInputs, nNeurons)\n",
    "        # Here shape of weights (4,3) is opp to previous block (3,4), reason to avoid extra transpose operation for every forward pass\n",
    "        self.biases = np.zeros((1, nNeurons))\n",
    "\n",
    "    def forward(self, inputs):\n",
    "        self.output = np.dot(inputs, self.weights) + self.biases\n",
    "\n",
    "\n",
    "# ReLU activation\n",
    "class ActivationReLU:\n",
    "    def forward(self, inputs):\n",
    "        self.output = np.maximum(0, inputs)\n",
    "\n",
    "\n",
    "# -----\n",
    "layer1 = LayerDense(2, 5)\n",
    "activation1 = ActivationReLU()\n",
    "\n",
    "layer1.forward(X)\n",
    "print(layer1.output[:5])\n",
    "\n",
    "activation1.forward(layer1.output)\n",
    "print(activation1.output[:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "f391ed9d-4f92-40ec-b0c1-045e2d9797a7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.33333334 0.33333334 0.33333334]\n",
      " [0.3333332  0.3333332  0.33333364]\n",
      " [0.3333329  0.33333293 0.3333342 ]\n",
      " [0.3333326  0.33333263 0.33333477]\n",
      " [0.33333233 0.3333324  0.33333528]]\n"
     ]
    }
   ],
   "source": [
    "# Softmax\n",
    "\n",
    "# Input -> Exponentiate -> Normalize (Average with diff e^input ) -> Output\n",
    "# Exponentiate + Normalize = Softmax\n",
    "\n",
    "import numpy as np\n",
    "import nnfs\n",
    "from nnfs.datasets import spiral_data\n",
    "\n",
    "nnfs.init()\n",
    "\n",
    "# Layer\n",
    "\n",
    "\n",
    "class LayerDense:\n",
    "    def __init__(self, nInputs, nNeurons):\n",
    "        self.weights = 0.01 * np.random.randn(nInputs, nNeurons)\n",
    "        # Here shape of weights (4,3) is opp to previous block (3,4), reason to avoid extra transpose operation for every forward pass\n",
    "        self.biases = np.zeros((1, nNeurons))\n",
    "\n",
    "    def forward(self, inputs):\n",
    "        self.output = np.dot(inputs, self.weights) + self.biases\n",
    "\n",
    "\n",
    "# ReLU activation\n",
    "class ActivationReLU:\n",
    "    def forward(self, inputs):\n",
    "        self.output = np.maximum(0, inputs)\n",
    "\n",
    "\n",
    "# Softmax activation\n",
    "class ActivationSoftmax:\n",
    "    def forward(self, inputs):\n",
    "        # axis: 0 -> columns, 1 -> rows\n",
    "        expValues = np.exp(inputs - np.max(inputs, axis=1, keepdims=True))\n",
    "        probabilities = expValues / np.sum(expValues, axis=1, keepdims=True)\n",
    "        self.output = probabilities\n",
    "\n",
    "\n",
    "# -----\n",
    "# Using nnfs spiral sample data of 100 samples of 3 different classes\n",
    "X, y = spiral_data(100, 3)\n",
    "layer1 = LayerDense(2, 3)\n",
    "activation1 = ActivationReLU()\n",
    "\n",
    "layer2 = LayerDense(3, 3)\n",
    "activation2 = ActivationSoftmax()\n",
    "\n",
    "layer1.forward(X)\n",
    "activation1.forward(layer1.output)\n",
    "layer2.forward(activation1.output)\n",
    "activation2.forward(layer2.output)\n",
    "print(activation2.output[:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "5c1575bd-360e-4494-b908-23c8d7d500d0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.35667494393873245\n",
      "0.35667494393873245\n"
     ]
    }
   ],
   "source": [
    "# Calculating loss with categorical cross entropy\n",
    "# -ve sum of the target value multiplied by the log of predicted value for each value in the distribution\n",
    "import math\n",
    "\n",
    "softmaxOutput = [0.7, 0.1, 0.2]  # probability / confidence\n",
    "targetOutput = [1, 0, 0]  # One hot vector\n",
    "\n",
    "loss = -(\n",
    "    math.log(softmaxOutput[0]) * targetOutput[0]\n",
    "    + math.log(softmaxOutput[1]) * targetOutput[1]\n",
    "    + math.log(softmaxOutput[2]) * targetOutput[2]\n",
    ")\n",
    "\n",
    "print(loss)\n",
    "\n",
    "loss = -math.log(softmaxOutput[0])\n",
    "print(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d17721b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
