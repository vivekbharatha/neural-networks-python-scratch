{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "c9f33d13-d910-4bfa-9ee7-7115348d624e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9.144\n"
     ]
    }
   ],
   "source": [
    "# Single neuron\n",
    "inputs = [1, 2.2, 3.6, 4.9]\n",
    "weights = [0.5, 0.3, 0.84, 0.4]\n",
    "bias = 3\n",
    "\n",
    "output = inputs[0]*weights[0] + inputs[1]*weights[1] + \\\n",
    "    inputs[2]*weights[2] + inputs[3]*weights[3] + bias\n",
    "print(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ac07dc41-21e5-4f5d-9c6f-3e6470754d53",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[9.144, 7.462, 12.206]\n"
     ]
    }
   ],
   "source": [
    "# 3 neurons layer\n",
    "inputs = [1, 2.2, 3.6, 4.9]\n",
    "weights1 = [0.5, 0.3, 0.84, 0.4]\n",
    "weights2 = [0.11, 0.31, 0.84, 0.54]\n",
    "weights3 = [0.85, 0.23, 0.34, 0.74]\n",
    "bias1 = 3\n",
    "bias2 = 1\n",
    "bias3 = 6\n",
    "\n",
    "output1 = inputs[0]*weights1[0] + inputs[1]*weights1[1] + \\\n",
    "    inputs[2]*weights1[2] + inputs[3]*weights1[3] + bias1\n",
    "output2 = inputs[0]*weights2[0] + inputs[1]*weights2[1] + \\\n",
    "    inputs[2]*weights2[2] + inputs[3]*weights2[3] + bias2\n",
    "output3 = inputs[0]*weights3[0] + inputs[1]*weights3[1] + \\\n",
    "    inputs[2]*weights3[2] + inputs[3]*weights3[3] + bias3\n",
    "\n",
    "print([output1, output2, output3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "93a5c5bf-b9cf-428f-b744-1ae480522518",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[9.144, 8.462, 12.206]\n"
     ]
    }
   ],
   "source": [
    "# Refactoring to 2d matrix - 3 neurons layer\n",
    "inputs = [1, 2.2, 3.6, 4.9]\n",
    "weights = [\n",
    "    [0.5, 0.3, 0.84, 0.4],\n",
    "    [0.11, 0.31, 0.84, 0.54],\n",
    "    [0.85, 0.23, 0.34, 0.74]\n",
    "]\n",
    "bias = [3, 2, 6]\n",
    "\n",
    "layerOutputs = []  # output of this layer of 3 neurons\n",
    "for neuronWeights, neuronBias in zip(weights, bias):\n",
    "    neuronOutput = 0  # output of current neuron\n",
    "    for input, weight in zip(inputs, neuronWeights):\n",
    "        neuronOutput += input * weight\n",
    "    neuronOutput += neuronBias\n",
    "    layerOutputs.append(neuronOutput)\n",
    "\n",
    "print(layerOutputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "a49d0cc7-7bd8-46a5-a875-1203278b8381",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nArray \\nlist: l = [1,2,3]\\nShape = (4,)\\nType = 1D Array, Vector\\n---------\\nlistOfList: lol = \\n[\\n    [1,2,3],\\n    [4,5,6]\\n]\\nShape = (2,4)\\nType =  2D Array, Matrix, List of Vectors\\n---------\\nlistOfListOfList: lolol = \\n[\\n    [\\n        [1,2,3,3],\\n        [4,5,6,2]\\n    ],\\n    [\\n        [3,7,8,3],\\n        [8,6,5,9]\\n    ],\\n    [\\n        [,4,1,3,1],\\n        [8,9,1,8]\\n    ]\\n]\\nShape = (3,2,4)\\nType = 3D Array\\n'"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Shapes\n",
    "\n",
    "\"\"\"\n",
    "Array \n",
    "list: l = [1,2,3]\n",
    "Shape = (4,)\n",
    "Type = 1D Array, Vector\n",
    "---------\n",
    "listOfList: lol = \n",
    "[\n",
    "    [1,2,3],\n",
    "    [4,5,6]\n",
    "]\n",
    "Shape = (2,4)\n",
    "Type =  2D Array, Matrix, List of Vectors\n",
    "---------\n",
    "listOfListOfList: lolol = \n",
    "[\n",
    "    [\n",
    "        [1,2,3,3],\n",
    "        [4,5,6,2]\n",
    "    ],\n",
    "    [\n",
    "        [3,7,8,3],\n",
    "        [8,6,5,9]\n",
    "    ],\n",
    "    [\n",
    "        [,4,1,3,1],\n",
    "        [8,9,1,8]\n",
    "    ]\n",
    "]\n",
    "Shape = (3,2,4)\n",
    "Type = 3D Array\n",
    "\"\"\"\n",
    "# Tensor is an object that can be represented as an array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ddff7de5-5de3-417f-a9df-07db489921da",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9.144\n",
      "[ 9.144  8.462 12.206]\n"
     ]
    }
   ],
   "source": [
    "# Dot Product OR Matrix Product\n",
    "import numpy as np\n",
    "\n",
    "# For single neuron\n",
    "inputs = [1.0, 2.2, 3.6, 4.9]\n",
    "weights = [0.5, 0.3, 0.84, 0.4]\n",
    "bias = 3.0\n",
    "\n",
    "output = np.dot(inputs, weights) + bias\n",
    "\n",
    "print(output)\n",
    "\n",
    "# For 3 neurons\n",
    "inputs = [1, 2.2, 3.6, 4.9]\n",
    "weights = [\n",
    "    [0.5, 0.3, 0.84, 0.4],\n",
    "    [0.11, 0.31, 0.84, 0.54],\n",
    "    [0.85, 0.23, 0.34, 0.74]\n",
    "]\n",
    "biases = [3, 2, 6]\n",
    "# output = np.dot(inputs, weights) + biases <-- This will throw error due shapes not aligned (4,) X (3,4)\n",
    "output = np.dot(weights, inputs) + biases # Shapes are aligned (3,4) X (4,)\n",
    "\n",
    "print(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "05d1a9d3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 9.144  8.462 12.206]\n",
      " [15.614 13.903 19.901]\n",
      " [12.494 10.423 12.501]]\n"
     ]
    }
   ],
   "source": [
    "# Batches (3 samples of 4 inputs) - this allows to calculate in parallel\n",
    "inputs = [\n",
    "    [1, 2.2, 3.6, 4.9],\n",
    "    [5.1, 3.2, 6.6, 8.9],\n",
    "    [3.1, 1.2, 8.6, 0.9]\n",
    "]\n",
    "\n",
    "weights = [\n",
    "    [0.5, 0.3, 0.84, 0.4],\n",
    "    [0.11, 0.31, 0.84, 0.54],\n",
    "    [0.85, 0.23, 0.34, 0.74]\n",
    "]\n",
    "biases = [3, 2, 6]\n",
    "# output = np.dot(inputs, weights) + biases -> Shapes are not aligned (3,4) X (3,4)\n",
    "\n",
    "output = np.dot(inputs, np.array(weights).T) + biases # Shapes are aligned (3,4) X (4,3)\n",
    "print(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "84336d19-b13d-4805-be91-2a47b7f44c9c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0.09671312 -0.0284731  -0.06391024]\n",
      " [ 0.19621234 -0.09116601 -0.15906814]\n",
      " [ 0.15557911  0.02731658 -0.09643673]]\n",
      "[[ 0.00054427 -0.00084794]\n",
      " [ 0.00121973 -0.00240239]\n",
      " [ 0.00083918 -0.00062095]]\n",
      "[[0.00054427 0.        ]\n",
      " [0.00121973 0.        ]\n",
      " [0.00083918 0.        ]]\n"
     ]
    }
   ],
   "source": [
    "# Hidden layers with activiation function\n",
    "import numpy as np\n",
    "\n",
    "X = [\n",
    "    [1, 2.2, 3.6, 4.9],\n",
    "    [5.1, 3.2, 6.6, 8.9],\n",
    "    [3.1, 1.2, 8.6, 0.9]\n",
    "]\n",
    "\n",
    "# Layer\n",
    "class LayerDense:\n",
    "    def __init__(self, nInputs, nNeurons):\n",
    "        self.weights = 0.01 * np.random.randn(nInputs, nNeurons)\n",
    "        # Here shape of weights (4,3) is opp to previous block (3,4), reason to avoid extra transpose operation for every forward pass\n",
    "        self.biases = np.zeros((1, nNeurons))\n",
    "    def forward(self, inputs):\n",
    "        self.output = np.dot(inputs, self.weights) + self.biases\n",
    "\n",
    "# ReLU activation\n",
    "class ActivationReLU:\n",
    "    def forward(self, inputs):\n",
    "        self.output = np.maximum(0, inputs)\n",
    "# -----\n",
    "layer1 = LayerDense(4,3)\n",
    "layer2 = LayerDense(3,2)\n",
    "activation1 = ActivationReLU()\n",
    "\n",
    "layer1.forward(X)\n",
    "print(layer1.output)\n",
    "\n",
    "layer2.forward(layer1.output)\n",
    "print(layer2.output)\n",
    "\n",
    "activation1.forward(layer2.output)\n",
    "print(activation1.output)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "951223ae-df7a-453e-9692-f9ac46b07a9e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0.0000000e+00  0.0000000e+00  0.0000000e+00  0.0000000e+00\n",
      "   0.0000000e+00]\n",
      " [-8.3581581e-05 -7.9040430e-05 -1.3345221e-04  4.6550449e-05\n",
      "   4.5684628e-06]\n",
      " [-2.3999445e-04  5.9346880e-06 -2.2480826e-04  2.0357311e-05\n",
      "   6.1002436e-05]\n",
      " [-4.1212194e-04  4.3767208e-04 -9.5322714e-05 -1.7302230e-04\n",
      "   1.9264895e-04]\n",
      " [-5.5660505e-04  5.2738853e-04 -1.7207881e-04 -2.0267766e-04\n",
      "   2.4708614e-04]]\n",
      "[[0.0000000e+00 0.0000000e+00 0.0000000e+00 0.0000000e+00 0.0000000e+00]\n",
      " [0.0000000e+00 0.0000000e+00 0.0000000e+00 4.6550449e-05 4.5684628e-06]\n",
      " [0.0000000e+00 5.9346880e-06 0.0000000e+00 2.0357311e-05 6.1002436e-05]\n",
      " [0.0000000e+00 4.3767208e-04 0.0000000e+00 0.0000000e+00 1.9264895e-04]\n",
      " [0.0000000e+00 5.2738853e-04 0.0000000e+00 0.0000000e+00 2.4708614e-04]]\n"
     ]
    }
   ],
   "source": [
    "# Hidden layers with activation functions\n",
    "import numpy as np\n",
    "import nnfs\n",
    "from nnfs.datasets import spiral_data\n",
    "\n",
    "nnfs.init()\n",
    "# Using nnfs spiral sample data of 100 samples of 3 different classes\n",
    "X,y = spiral_data(100, 3)\n",
    "\n",
    "# Layer\n",
    "class LayerDense:\n",
    "    def __init__(self, nInputs, nNeurons):\n",
    "        self.weights = 0.01 * np.random.randn(nInputs, nNeurons)\n",
    "        # Here shape of weights (4,3) is opp to previous block (3,4), reason to avoid extra transpose operation for every forward pass\n",
    "        self.biases = np.zeros((1, nNeurons))\n",
    "    def forward(self, inputs):\n",
    "        self.output = np.dot(inputs, self.weights) + self.biases\n",
    "\n",
    "# ReLU activation\n",
    "class ActivationReLU:\n",
    "    def forward(self, inputs):\n",
    "        self.output = np.maximum(0, inputs)\n",
    "# -----\n",
    "layer1 = LayerDense(2,5)\n",
    "activation1 = ActivationReLU()\n",
    "\n",
    "layer1.forward(X)\n",
    "print(layer1.output[:5])\n",
    "\n",
    "activation1.forward(layer1.output)\n",
    "print(activation1.output[:5])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "f391ed9d-4f92-40ec-b0c1-045e2d9797a7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.33333334 0.33333334 0.33333334]\n",
      " [0.3333332  0.3333332  0.33333364]\n",
      " [0.3333329  0.33333293 0.3333342 ]\n",
      " [0.3333326  0.33333263 0.33333477]\n",
      " [0.33333233 0.3333324  0.33333528]]\n"
     ]
    }
   ],
   "source": [
    "# Softmax\n",
    "\n",
    "# Input -> Exponentiate -> Normalize (Average with diff e^input ) -> Output\n",
    "# Exponentiate + Normalize = Softmax\n",
    "\n",
    "import numpy as np\n",
    "import nnfs\n",
    "from nnfs.datasets import spiral_data\n",
    "\n",
    "nnfs.init()\n",
    "\n",
    "# Layer\n",
    "class LayerDense:\n",
    "    def __init__(self, nInputs, nNeurons):\n",
    "        self.weights = 0.01 * np.random.randn(nInputs, nNeurons)\n",
    "        # Here shape of weights (4,3) is opp to previous block (3,4), reason to avoid extra transpose operation for every forward pass\n",
    "        self.biases = np.zeros((1, nNeurons))\n",
    "    def forward(self, inputs):\n",
    "        self.output = np.dot(inputs, self.weights) + self.biases\n",
    "\n",
    "# ReLU activation\n",
    "class ActivationReLU:\n",
    "    def forward(self, inputs):\n",
    "        self.output = np.maximum(0, inputs)\n",
    "\n",
    "# Softmax activation\n",
    "class ActivationSoftmax:\n",
    "    def forward(self, inputs):\n",
    "        expValues = np.exp(inputs - np.max(inputs, axis=1, keepdims=True)) # axis: 0 -> columns, 1 -> rows\n",
    "        probabilities = expValues / np.sum(expValues, axis=1, keepdims=True)\n",
    "        self.output = probabilities\n",
    "\n",
    "# -----\n",
    "# Using nnfs spiral sample data of 100 samples of 3 different classes\n",
    "X,y = spiral_data(100, 3)\n",
    "layer1 = LayerDense(2,3)\n",
    "activation1 = ActivationReLU()\n",
    "\n",
    "layer2 = LayerDense(3, 3)\n",
    "activation2 = ActivationSoftmax()\n",
    "\n",
    "layer1.forward(X)\n",
    "activation1.forward(layer1.output)\n",
    "layer2.forward(activation1.output)\n",
    "activation2.forward(layer2.output)\n",
    "print(activation2.output[:5])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
